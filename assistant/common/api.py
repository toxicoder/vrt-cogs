import asyncio
import google.auth
import google.auth.transport.requests
import inspect
import json
import logging
import math
from typing import List, Optional

import aiohttp
import discord
import tiktoken
from openai.types.chat.chat_completion import ChatCompletion
from openai.types.chat.chat_completion_message import ChatCompletionMessage
# Used to help adapt Gemini response to be somewhat OpenAI-like for easier integration
from openai.types.chat.chat_completion import Choice as OpenAIChoice, ChatCompletion as OpenAIChatCompletion
from openai.types.chat.chat_completion_message_tool_call import ChatCompletionMessageToolCall
from openai.types.chat.chat_completion_chunk import ChoiceDeltaToolCall, ChoiceDeltaToolCallFunction
from openai.types.chat.completion_create_params import Function as OpenAIFunction
from openai.types.create_embedding_response import CreateEmbeddingResponse, Usage as OpenAIEmbeddingUsage
from openai.types.embedding import Embedding as OpenAIEmbedding
from redbot.core import commands
from redbot.core.i18n import Translator, cog_i18n
from redbot.core.utils.chat_formatting import box, humanize_number

from ..abc import MixinMeta
from .calls import request_chat_completion_raw, request_embedding_raw
from .constants import MODELS
from .models import GuildSettings

log = logging.getLogger("red.vrt.assistant.api")
_ = Translator("Assistant", __file__)


@cog_i18n(_)
class API(MixinMeta):
    async def openai_status(self) -> str:
        try:
            timeout = aiohttp.ClientTimeout(total=5)
            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.get(url="https://status.openai.com/api/v2/status.json") as res:
                    data = await res.json()
                    status = data["status"]["description"]
                    # ind = data["status"]["indicator"]
        except Exception as e:
            log.error("Failed to fetch OpenAI API status", exc_info=e)
            status = _("Failed to fetch: {}").format(str(e))
        return status

    async def request_response(
        self,
        messages: List[dict],
        conf: GuildSettings,
        functions: Optional[List[dict]] = None,
        member: Optional[discord.Member] = None,
        response_token_override: int = None,
        model_override: Optional[str] = None,
        temperature_override: Optional[float] = None,
    ) -> ChatCompletionMessage:
        model_name = model_override or conf.get_user_model(member)
        
        # Determine if we are using Gemini
        is_gemini = model_name.startswith("gemini-")

        # API key and base URL determination
        api_key_to_use = None
        project_id_to_use = None
        base_url_to_use = self.db.endpoint_override # Default, can be overridden for Gemini
        using_aistudio_key = False

        if is_gemini:
            if self.db.google_ai_studio_api_key:
                api_key_to_use = self.db.google_ai_studio_api_key
                base_url_to_use = "https://generativelanguage.googleapis.com/v1beta"
                project_id_to_use = None # Not used for AI Studio keys
                using_aistudio_key = True
                log.debug("Using Google AI Studio API Key for chat")
            elif self.db.gemini_api_key: # Keep this for backwards compatibility or separate studio key
                api_key_to_use = self.db.gemini_api_key
                base_url_to_use = "https://generativelanguage.googleapis.com/v1beta"
                project_id_to_use = None # Not used for AI Studio keys
                using_aistudio_key = True
                log.debug("Using Gemini (Older Style) AI Studio API Key for chat")
            else:
                using_aistudio_key = False
                try:
                    credentials, detected_project_id = await asyncio.to_thread(
                        google.auth.default, scopes=["https://www.googleapis.com/auth/cloud-platform"]
                    )
                    if not credentials:
                        raise google.auth.exceptions.DefaultCredentialsError(_("Google Cloud credentials not found."))

                    # Refresh credentials if they are stale
                    if credentials.expired and credentials.refresh_token:
                         await asyncio.to_thread(credentials.refresh, google.auth.transport.requests.Request())

                    api_key_to_use = credentials.token # This is the Google Access Token
                except Exception as e:
                    log.error("Failed to get Google Cloud credentials for Gemini", exc_info=e)
                    raise commands.UserFeedbackCheckFailure(
                        _("Gemini Auth Error: Could not obtain Google Cloud credentials. Ensure Application Default Credentials are set up correctly or provide an AI Studio API key. Error: {}").format(e)
                    ) from e

                project_id_to_use = self.db.google_project_id or detected_project_id
                if not project_id_to_use:
                    raise commands.UserFeedbackCheckFailure(
                        _("Google Cloud Project ID not set or detected. Configure it using the bot's admin commands or ensure ADC is providing it.")
                    )
                # For Gemini with ADC, base_url is regional.
                base_url_to_use = self.db.endpoint_override or "https://us-central1-aiplatform.googleapis.com/v1"
                log.debug(f"Using Google Cloud ADC for Gemini chat. Project: {project_id_to_use}, Endpoint: {base_url_to_use}")
        else:
            api_key_to_use = conf.api_key
            base_url_to_use = self.db.endpoint_override


        max_convo_tokens = self.get_max_tokens(conf, member) # Max tokens for the *conversation*
        max_response_tokens_user_setting = conf.get_user_max_response_tokens(member) # User's desired max for the *response*

        current_convo_tokens = await self.count_payload_tokens(messages, model_name)
        if functions:
            current_convo_tokens += await self.count_function_tokens(functions, model_name)

        # Dynamically adjust OpenAI model_name to lower cost if applicable
        if not is_gemini:
            if "-16k" in model_name and current_convo_tokens < 3000:
                model_name = model_name.replace("-16k", "")
            if "-32k" in model_name and current_convo_tokens < 4000:
                model_name = model_name.replace("-32k", "")

        max_model_tokens = MODELS.get(model_name, 1000000 if is_gemini else 4096) # Default based on type

        # Calculate actual max_tokens for the API call (max_output_tokens for Gemini)
        # This needs to be the capacity for the response itself.
        if response_token_override:
            max_api_response_tokens = response_token_override
        elif max_response_tokens_user_setting:
             # User has a specific limit for the response size
            max_api_response_tokens = max_response_tokens_user_setting
        else:
            # Dynamic: calculate available space, ensuring it's positive
            available_for_response = max_model_tokens - current_convo_tokens
            max_api_response_tokens = max(0, available_for_response) if not is_gemini else None # Gemini often doesn't need this if not strictly limiting output

        if model_name not in MODELS: # Check against our known models list
            log.warning(f"Model {model_name} is not in internal MODELS list. Attempting to use, but may fail.")
            # Potentially switch to a default if truly unknown, but for now, let it try

        if is_gemini:
            # `model_name` for Gemini is just the model ID, e.g., "gemini-1.5-pro-latest"
            # `project_id_to_use` is determined above
            # `base_url_to_use` is the regional Vertex AI endpoint
            
            # Extract system message if present (OpenAI format uses first message if role is system)
            system_message_str = None
            processed_messages = messages
            if messages and messages[0]["role"] == "system":
                system_message_str = messages[0]["content"]
                processed_messages = messages[1:] # Pass messages without system prompt if extracted

            gemini_raw_response = await self.request_gemini_chat_completion_raw(
                model=model_name, # Just the model ID, e.g., "gemini-1.5-pro-latest"
                project_id=project_id_to_use,
                messages=processed_messages, 
                temperature=temperature_override if temperature_override is not None else conf.temperature,
                api_key=api_key_to_use, # Google Cloud Access Token
                max_tokens=max_api_response_tokens, 
                functions=functions,
                base_url=base_url_to_use,
                system_message=system_message_str,
                using_aistudio_key=using_aistudio_key,
                # seed=conf.seed, # TODO: Add seed if supported
            )
            
            # Adapt Gemini response to OpenAI's ChatCompletionMessage structure
            # This is a simplified adaptation
            final_content = ""
            tool_calls_adapted = []

            if gemini_raw_response.get("candidates"):
                candidate = gemini_raw_response["candidates"][0]
                if candidate.get("content") and candidate["content"].get("parts"):
                    for part in candidate["content"]["parts"]:
                        if "text" in part:
                            final_content += part["text"]
                        elif "functionCall" in part:
                            fc = part["functionCall"]
                            tool_calls_adapted.append(
                                ChatCompletionMessageToolCall(
                                    id=f"call_{fc['name']}_{abs(hash(json.dumps(fc['args'])))}", # Create a unique enough ID
                                    function=OpenAIFunction(name=fc["name"], arguments=json.dumps(fc["args"])),
                                    type="function",
                                )
                            )
            
            message_role = "assistant" # Gemini uses "model" for assistant responses
            
            prompt_tokens = gemini_raw_response.get("usageMetadata", {}).get("promptTokenCount", 0)
            completion_tokens = gemini_raw_response.get("usageMetadata", {}).get("candidatesTokenCount", 0)
            total_tokens = gemini_raw_response.get("usageMetadata", {}).get("totalTokenCount", 0)

            adapted_choice = OpenAIChoice(
                finish_reason="stop", # Gemini has finishReason, map it if needed
                index=0,
                message=ChatCompletionMessage(
                    content=final_content if final_content else None, 
                    role=message_role, 
                    tool_calls=tool_calls_adapted if tool_calls_adapted else None,
                    function_call=None
                ),
                logprobs=None,
            )
            
            simulated_openai_response = OpenAIChatCompletion(
                id="chatcmpl-gemini-" + str(abs(hash(final_content))), 
                choices=[adapted_choice],
                created=int(discord.utils.utcnow().timestamp()), 
                model=model_name, 
                object="chat.completion", 
                usage={
                    "prompt_tokens": prompt_tokens,
                    "completion_tokens": completion_tokens,
                    "total_tokens": total_tokens,
                }
            )
            message_to_return = simulated_openai_response.choices[0].message
            
            conf.update_usage(
                model_name, 
                total_tokens,
                prompt_tokens,
                completion_tokens,
            )

        else: # OpenAI or compatible API call
            if model_name not in MODELS: 
                log.error(f"OpenAI model {model_name} is not in internal MODELS list. Switching to gpt-4o-mini.")
                model_name = "gpt-4o-mini"

            response: OpenAIChatCompletion = await request_chat_completion_raw(
                model=model_name,
                messages=messages,
                temperature=temperature_override if temperature_override is not None else conf.temperature,
                api_key=api_key_to_use,
                max_tokens=max_api_response_tokens,
                functions=functions,
                frequency_penalty=conf.frequency_penalty,
                presence_penalty=conf.presence_penalty,
                seed=conf.seed,
                base_url=base_url_to_use,
            )
            message_to_return: ChatCompletionMessage = response.choices[0].message
            conf.update_usage(
                response.model,
                response.usage.total_tokens,
                response.usage.prompt_tokens,
                response.usage.completion_tokens,
            )

        log.debug(f"MESSAGE TYPE: {type(message_to_return)}")
        return message_to_return

    async def request_embedding(self, text: str, conf: GuildSettings) -> List[float]:
        model_name = conf.embed_model
        is_gemini_provider = model_name.startswith("gemini-")
        
        api_key_to_use = None
        project_id_to_use = None
        base_url_to_use = self.db.endpoint_override
        using_aistudio_key = False

        if is_gemini_provider:
            if self.db.google_ai_studio_api_key:
                api_key_to_use = self.db.google_ai_studio_api_key
                base_url_to_use = "https://generativelanguage.googleapis.com/v1beta"
                project_id_to_use = None # Not used for AI Studio keys
                using_aistudio_key = True
                log.debug("Using Google AI Studio API Key for embeddings")
            elif self.db.gemini_api_key: # Keep this for backwards compatibility
                api_key_to_use = self.db.gemini_api_key
                base_url_to_use = "https://generativelanguage.googleapis.com/v1beta"
                project_id_to_use = None # Not used for AI Studio keys
                using_aistudio_key = True
                log.debug("Using Gemini (Older Style) AI Studio API Key for embeddings")
            else:
                using_aistudio_key = False
                try:
                    credentials, detected_project_id = await asyncio.to_thread(
                        google.auth.default, scopes=["https://www.googleapis.com/auth/cloud-platform"]
                    )
                    if not credentials:
                        raise google.auth.exceptions.DefaultCredentialsError(_("Google Cloud credentials not found for Gemini embeddings."))
                    if credentials.expired and credentials.refresh_token:
                        await asyncio.to_thread(credentials.refresh, google.auth.transport.requests.Request())
                    api_key_to_use = credentials.token
                except Exception as e:
                    log.error("Failed to get Google Cloud credentials for Gemini embedding", exc_info=e)
                    raise commands.UserFeedbackCheckFailure(
                         _("Gemini Auth Error (Embeddings): Could not obtain Google Cloud credentials or use AI Studio Key. Error: {}").format(e)
                    ) from e

                project_id_to_use = self.db.google_project_id or detected_project_id
                if not project_id_to_use:
                    raise commands.UserFeedbackCheckFailure(
                        _("Google Cloud Project ID not set or detected for Gemini embeddings. Configure it or ensure ADC provides it.")
                    )
                base_url_to_use = self.db.endpoint_override or f"https://us-central1-aiplatform.googleapis.com/v1"
                log.debug(f"Using Google Cloud ADC for Gemini embeddings. Project: {project_id_to_use}, Endpoint: {base_url_to_use}")

            gemini_response = await self.request_gemini_embedding_raw(
                text=text,
                api_key=api_key_to_use, 
                model=model_name,
                project_id=project_id_to_use, # May be None if using AI Studio Key
                base_url=base_url_to_use,
                using_aistudio_key=using_aistudio_key
            )
            # Handle different response structures for AI Studio vs Vertex AI
            if using_aistudio_key:
                # AI Studio direct embedding response
                if "embedding" not in gemini_response or "value" not in gemini_response["embedding"]:
                    log.error(f"Invalid Gemini AI Studio Embedding API response format: {gemini_response}")
                    raise commands.UserFeedbackCheckFailure(_("Invalid response format from Gemini AI Studio Embedding API."))
                embedding_values = gemini_response["embedding"]["value"]
                # AI Studio does not provide token counts in the embedding response directly
                token_count = await self.count_tokens(text, model_name)
            else:
                # Vertex AI embedding response
                if not gemini_response.get("predictions") or not gemini_response["predictions"][0].get("embeddings"):
                    log.error(f"Invalid Vertex AI Embedding API response format: {gemini_response}")
                    raise commands.UserFeedbackCheckFailure(_("Invalid response format from Vertex AI Embedding API."))
                embedding_values = gemini_response["predictions"][0]["embeddings"]["values"]
                token_count = gemini_response["predictions"][0]["embeddings"]["statistics"]["token_count"]
            
            conf.update_usage(
                model_name, 
                token_count,
                token_count, 
                0, 
            )
            return embedding_values
        else: # OpenAI or compatible
            api_key_to_use = conf.api_key # Standard OpenAI key
            response: CreateEmbeddingResponse = await self.request_openai_embedding_raw(
                text=text,
                api_key=api_key_to_use,
                model=model_name,
                base_url=base_url_to_use,
            )
            conf.update_usage(
                response.model,
                response.usage.total_tokens,
                response.usage.prompt_tokens,
                0,
            )
            return response.data[0].embedding

    async def request_gemini_embedding_raw(
        self,
        text: str,
        api_key: str,
        model: str,
        project_id: Optional[str], # Google Cloud Project ID, optional if using AI Studio key
        base_url: str,
        task_type: str = "RETRIEVAL_DOCUMENT", 
        using_aistudio_key: bool = False,
    ) -> dict: 
        """
        Makes a raw request to a Google Gemini Embedding model (Vertex AI or AI Studio).
        """
        # Construct the full model path for the endpoint
        # Example: projects/PROJECT_ID/locations/us-central1/publishers/google/models/text-embedding-004
        # Assuming 'model' is just the ID like 'text-embedding-004' or 'gemini-1.5-flash' for embeddings
        # For embeddings, the model path often includes "publishers/google/models/"
        # We'll assume a default location `us-central1` if not part of base_url or model string
        location = "us-central1" # Or extract from base_url if more complex logic is needed
        
        if using_aistudio_key:
            # AI Studio uses a different endpoint structure and payload
            # Model is usually just "embedding-001" or similar.
            # Base URL: "https://generativelanguage.googleapis.com/v1beta"
            # Key is passed in the URL for REST embedContent.
            predict_endpoint = f"{base_url}/models/{model}:embedContent?key={api_key}"
            payload = {"content": {"parts": [{"text": text}]}} # Model is in the URL, not payload
            headers = {
                "Content-Type": "application/json; charset=utf-8",
            }
        else:
            # Vertex AI
            if not project_id: # Should be ensured by caller if not using AI studio key
                raise ValueError("Project ID is required for Vertex AI Gemini embeddings.")

            if model.startswith("projects/"):
                model_path_for_endpoint = model
            else:
                model_path_for_endpoint = f"projects/{project_id}/locations/{location}/publishers/google/models/{model}"

            predict_endpoint = f"{base_url}/{model_path_for_endpoint}:predict"
            payload = {
                "instances": [{"content": text, "task_type": task_type}],
            }
            headers = { # Vertex AI uses Bearer token
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json; charset=utf-8",
            }

        timeout_obj = aiohttp.ClientTimeout(total=120)
        # Assuming self.session is the cog's session, managed elsewhere
        async with self.session.post(predict_endpoint, json=payload, headers=headers, timeout=timeout_obj) as resp:
            # async with session.post(predict_endpoint, json=payload, headers=headers) as resp: # Original line if using local session
                # async with self.bot.session.post(predict_endpoint, json=payload, headers=headers, timeout=timeout) as resp: # Original line if using local session
                    if resp.status != 200:
                        err_text = await resp.text()
                        log.error(f"Gemini Embedding API Error ({resp.status}) hitting {predict_endpoint}: {err_text}")
                        raise commands.UserFeedbackCheckFailure(
                            _("Gemini Embedding API request failed with status {status}: {error}").format(status=resp.status, error=err_text)
                        )
                        
                    response_json = await resp.json()
            
        return response_json

    async def request_openai_embedding_raw(self, text: str, api_key: str, model: str, base_url: Optional[str] = None ) -> CreateEmbeddingResponse:
        return await request_embedding_raw( 
            text=text,
            api_key=api_key,
            model=model,
            base_url=base_url,
        )

    async def request_gemini_chat_completion_raw(
        self,
        model: str,
        project_id: Optional[str], # Google Cloud Project ID, optional if using AI Studio key
        messages: List[dict],
        temperature: float,
        api_key: str,
        max_tokens: Optional[int], 
        functions: Optional[List[dict]], 
        base_url: str,
        system_message: Optional[str] = None,
        using_aistudio_key: bool = False,
    ) -> dict: 
        """
        Makes a raw request to a Google Gemini Chat Completion model (Vertex AI or AI Studio).
        """
        # Construct the full model path for the endpoint
        # Example: projects/PROJECT_ID/locations/us-central1/publishers/google/models/gemini-1.5-pro-latest
        location = "us-central1" # Or extract from base_url
        
        action = "generateContent" # Could be streamGenerateContent for streaming

        if using_aistudio_key:
            # AI Studio uses a different endpoint structure. Key is in the URL.
            model_id_for_url = model if model.startswith("models/") else f"models/{model}"
            generate_endpoint = f"{base_url}/{model_id_for_url}:{action}?key={api_key}"
            headers = {
                "Content-Type": "application/json; charset=utf-8",
            }
        else:
            # Vertex AI
            if not project_id:
                raise ValueError("Project ID is required for Vertex AI Gemini chat.")

            if model.startswith("projects/"):
                model_path_for_endpoint = model
            else:
                model_path_for_endpoint = f"projects/{project_id}/locations/{location}/publishers/google/models/{model}"
            generate_endpoint = f"{base_url}/{model_path_for_endpoint}:{action}"
            headers = {
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json; charset=utf-8",
            }

        gemini_contents = []
        for msg in messages:
            role = "user" if msg["role"] == "user" else "model" 
            
            if msg["role"] == "tool" or msg["role"] == "function": 
                gemini_contents.append({
                    "role": "function", 
                    "parts": [{
                        "functionResponse": {
                            "name": msg.get("name") or msg.get("tool_call_id", "unknown_function"), 
                            "response": {"content": msg["content"]}, 
                        }
                    }]
                })
                continue

            if isinstance(msg.get("content"), list): 
                parts = []
                for item in msg["content"]:
                    if item["type"] == "text":
                        parts.append({"text": item["text"]})
                    elif item["type"] == "image_url":
                        image_url_data = item["image_url"]["url"]
                        if image_url_data.startswith("data:image/"): 
                            mime_type, base64_data = image_url_data.split(";", 1)[0].split(":")[1], image_url_data.split(",", 1)[1]
                            parts.append({"inlineData": {"mimeType": mime_type, "data": base64_data}})
                        else: 
                            log.warning(f"Direct image URL {image_url_data} might not be supported by Gemini API. Consider inlineData or fileData.")
                            parts.append({"text": f"[Image available at: {image_url_data}]"}) 
                gemini_contents.append({"role": role, "parts": parts})
            elif isinstance(msg.get("content"), str): 
                 gemini_contents.append({"role": role, "parts": [{"text": msg["content"]}]})

        payload = {"contents": gemini_contents}
        
        generation_config = {}
        if temperature is not None:
            generation_config["temperature"] = temperature
        if max_tokens is not None: 
            generation_config["maxOutputTokens"] = max_tokens

        if generation_config:
            payload["generationConfig"] = generation_config

        if system_message:
            payload["systemInstruction"] = {"parts": [{"text": system_message}]} # AI Studio format for system prompt

        if functions: 
            gemini_tools = []
            function_declarations = []
            for func_schema in functions:
                # Ensure parameters is a dict, even if empty, for Gemini schema
                params = func_schema.get("parameters")
                if params is None or not isinstance(params, dict) or not params.get("properties"):
                     # Gemini needs parameters to be a valid OpenAPI schema object.
                     # If it's missing or not structured correctly, provide a default empty one.
                    params = {"type": "object", "properties": {}}

                function_declarations.append({
                    "name": func_schema["name"],
                    "description": func_schema.get("description", ""),
                    "parameters": params,
                })

            if function_declarations:
                 gemini_tools.append({"functionDeclarations": function_declarations})
            if gemini_tools:
                payload["tools"] = gemini_tools
        
        # Headers are defined above based on using_aistudio_key

        timeout = aiohttp.ClientTimeout(total=300) 
        async with self.session.post(generate_endpoint, json=payload, headers=headers, timeout=timeout) as resp:
            if resp.status != 200:
                err_text = await resp.text()
                log.error(f"Gemini API Error ({resp.status}) hitting {generate_endpoint}: {err_text}\nPayload: {json.dumps(payload, indent=2)}")
                raise commands.UserFeedbackCheckFailure(
                     _("Gemini API request failed with status {status}: {error}").format(status=resp.status, error=err_text)
                )
            try:
                response_json = await resp.json()
            except aiohttp.ContentTypeError:
                err_text = await resp.text()
                log.error(f"Gemini API Error: Non-JSON response from {generate_endpoint}. Response: {err_text}")
                raise commands.UserFeedbackCheckFailure(
                    _("Gemini API returned a non-JSON response.")
                )

            if not response_json.get("candidates") and not response_json.get("error"):
                if response_json.get("promptFeedback", {}).get("blockReason"):
                    block_reason = response_json["promptFeedback"]["blockReason"]
                    log.warning(f"Gemini request blocked. Reason: {block_reason}. Details: {response_json['promptFeedback']}")
                    raise commands.UserFeedbackCheckFailure(
                        _("Content blocked by Gemini due to: {reason}").format(reason=block_reason)
                    )
                log.error(f"Invalid Gemini API response format (no candidates or error): {response_json} from {generate_endpoint}")
                raise commands.UserFeedbackCheckFailure(_("Invalid response format from Gemini API (no candidates or error field)."))
            elif response_json.get("error"):
                error_details = response_json["error"]
                log.error(f"Gemini API returned an error: {error_details} from {generate_endpoint}")
                raise commands.UserFeedbackCheckFailure(
                    _("Gemini API error: {message} (Code: {code})").format(message=error_details.get("message","Unknown"), code=error_details.get("code", "N/A"))
                )

            return response_json

    # -------------------------------------------------------
    # -------------------------------------------------------
    # ----------------------- HELPERS -----------------------
    # -------------------------------------------------------
    # -------------------------------------------------------

    async def count_payload_tokens(self, messages: List[dict], model_name: str = "gpt-4o-mini") -> int:
        if model_name.startswith("gemini-"):
            num_tokens = 0
            for message in messages:
                content = message.get("content")
                if isinstance(content, str):
                    num_tokens += len(content) 
                elif isinstance(content, list): 
                    for item in content:
                        if item["type"] == "text":
                             num_tokens += len(item["text"]) 
            log.debug(f"Rough token estimate for Gemini model '{model_name}': {num_tokens} (character count based)")
            return num_tokens 

        if not messages:
            return 0

        def _count_payload_openai():
            try:
                encoding = tiktoken.encoding_for_model(model_name)
            except KeyError:
                log.warning(f"Tiktoken encoding not found for model {model_name}. Using o200k_base.")
                encoding = tiktoken.get_encoding("o200k_base")

            tokens_per_message = 3
            tokens_per_name = 1
            num_tokens = 0
            for message in messages:
                num_tokens += tokens_per_message
                for key, value in message.items():
                    if key == "content" and value is None: 
                        continue
                    if isinstance(value, list) and key == "content": 
                        for item in value:
                            if item.get("type") == "text" and item.get("text"):
                                num_tokens += len(encoding.encode(str(item["text"])))
                    elif isinstance(value, str):
                         num_tokens += len(encoding.encode(value))
                    else: 
                        num_tokens += len(encoding.encode(json.dumps(value)))

                    if key == "name":
                        num_tokens += tokens_per_name
            num_tokens += 3  
            return num_tokens

        return await asyncio.to_thread(_count_payload_openai)

    async def count_function_tokens(self, functions: List[dict], model_name: str = "gpt-4o-mini") -> int:
        if model_name.startswith("gemini-"):
            if not functions:
                return 0
            try:
                json_str = json.dumps(functions)
                log.debug(f"Rough function token estimate for Gemini model '{model_name}': {len(json_str)} (character count of JSON)")
                return len(json_str) 
            except Exception as e:
                log.error(f"Error estimating function tokens for Gemini: {e}")
                return 0 

        if not functions:
            return 0
            
        func_init = 0
        prop_init = 0
        prop_key = 0
        enum_init = 0
        enum_item = 0
        func_end = 0

        if model_name in [
            "gpt-4o",
            "gpt-4o-2024-05-13",
            "gpt-4o-2024-08-06",
            "gpt-4o-2024-11-20",
            "gpt-4o-mini",
            "gpt-4o-mini-2024-07-18",
            "gpt-4.1",
            "gpt-4.1-mini",
            "gpt-4.1-nano",
            "o1-preview",
            "o1-preview-2024-09-12",
            "o1",
            "o1-2024-12-17",
            "o1-mini",
            "o1-mini-2024-09-12",
            "o3-mini",
            "o3-mini-2025-01-31",
        ]:
            func_init = 7
            prop_init = 3
            prop_key = 3
            enum_init = -3
            enum_item = 3
            func_end = 12
        elif model_name in [
            "gpt-3.5-turbo-1106",
            "gpt-3.5-turbo-0125",
            "gpt-4",
            "gpt-4-turbo",
            "gpt-4-turbo-preview",
            "gpt-4-0125-preview",
            "gpt-4-1106-preview",
        ] or model_name.startswith("gpt-4o") or model_name.startswith("gpt-4.1"): 
            func_init = 10
            prop_init = 3
            prop_key = 3
            enum_init = -3
            enum_item = 3
            func_end = 12
        else:
            log.warning(f"Incompatible model for function token counting: {model_name}")

        def _count_tokens():
            try:
                encoding = tiktoken.encoding_for_model(model_name)
            except KeyError:
                encoding = tiktoken.get_encoding("o200k_base")

            func_token_count = 0
            if len(functions) > 0:
                for f in functions:
                    if "function" not in f.keys():
                        f = {"function": f, "name": f["name"], "description": f["description"]}
                    func_token_count += func_init  
                    function = f["function"]
                    f_name = function["name"]
                    f_desc = function["description"]
                    if f_desc.endswith("."):
                        f_desc = f_desc[:-1]
                    line = f_name + ":" + f_desc
                    func_token_count += len(encoding.encode(line))  
                    if len(function["parameters"]["properties"]) > 0:
                        func_token_count += prop_init  
                        for key in list(function["parameters"]["properties"].keys()):
                            func_token_count += prop_key  
                            p_name = key
                            p_type = function["parameters"]["properties"][key].get("type", "")
                            p_desc = function["parameters"]["properties"][key].get("description", "")
                            if "enum" in function["parameters"]["properties"][key].keys():
                                func_token_count += enum_init  
                                for item in function["parameters"]["properties"][key]["enum"]:
                                    func_token_count += enum_item
                                    func_token_count += len(encoding.encode(item))
                            if p_desc.endswith("."):
                                p_desc = p_desc[:-1]
                            line = f"{p_name}:{p_type}:{p_desc}"
                            func_token_count += len(encoding.encode(line))
                func_token_count += func_end
            return func_token_count
        return await asyncio.to_thread(_count_tokens)

    async def get_tokens(self, text: str, model_name: str = "gpt-4o-mini") -> list[int]:
        if not text:
            log.debug("No text to get tokens from!")
            return []
        if isinstance(text, bytes):
            text = text.decode(encoding="utf-8", errors="ignore") 

        if model_name.startswith("gemini-"):
            log.warning(f"get_tokens called for Gemini model '{model_name}'. Returning character codes as placeholder.")
            return [ord(c) for c in text]

        def _get_encoding_openai():
            try:
                enc = tiktoken.encoding_for_model(model_name)
            except KeyError:
                log.warning(f"Tiktoken encoding not found for model {model_name}. Using o200k_base.")
                enc = tiktoken.get_encoding("o200k_base")
            return enc

        encoding = await asyncio.to_thread(_get_encoding_openai)
        return await asyncio.to_thread(encoding.encode, text)

    async def count_tokens(self, text: str, model_name: str) -> int:
        if not text:
            log.debug("No text to get token count from!")
            return 0
        
        if model_name.startswith("gemini-"):
            char_count = len(text)
            log.debug(f"Rough token estimate for Gemini model '{model_name}' (text): {char_count} (character count)")
            return char_count 

        try:
            tokens = await self.get_tokens(text, model_name) 
            return len(tokens)
        except TypeError as e:
            log.error(f"Failed to count tokens for '{model_name}': {text}", exc_info=e)
            return 0

    async def can_call_llm(self, conf: GuildSettings, ctx: Optional[commands.Context] = None) -> bool:
        model_name = conf.get_user_model(ctx.author if ctx else None)
        is_gemini = model_name.startswith("gemini-")

        if is_gemini:
            # Priority 1: Google AI Studio API Key (new)
            if self.db.google_ai_studio_api_key:
                return True
            # Priority 2: Older Gemini API Key (for AI Studio, backward compatibility)
            if self.db.gemini_api_key:
                return True

            # Priority 3: Google Cloud ADC (Vertex AI)
            # Check if a project ID is configured, as it's needed for Vertex AI calls via ADC
            if not self.db.google_project_id:
                if ctx:
                    await ctx.send(
                        _(
                            "For Gemini models, you need to set either a Google AI Studio API key, "
                            "a Gemini API Key (older style for AI Studio), or a Google Cloud Project ID "
                            "for use with Application Default Credentials (ADC).\n"
                            "- Set Google AI Studio Key: `{prefix}assistant setgaistudiokey <key>`\n"
                            "- Set Google Cloud Project ID: `{prefix}assistant gauth <project_id>`"
                        ).format(prefix=ctx.clean_prefix)
                    )
                return False
            try:
                # Try to get credentials to check if ADC is set up for Vertex AI
                credentials, _ = await asyncio.to_thread(google.auth.default, scopes=["https://www.googleapis.com/auth/cloud-platform"])
                if not credentials:
                    if ctx:
                        await ctx.send(
                            _(
                                "Google Cloud credentials not found for Vertex AI, and no direct AI Studio key is set. "
                                "Ensure Application Default Credentials (ADC) are configured in the bot's environment, "
                                "or set a Google AI Studio API key."
                            )
                        )
                    return False
            except Exception as e:
                if ctx:
                    await ctx.send(
                        _(
                            "Failed to acquire Google Cloud credentials for Vertex AI (and no AI Studio key is set): {error}. "
                            "Ensure ADC is configured or set an AI Studio API key."
                        ).format(error=e)
                    )
                log.error("ADC check failed for Gemini (no AI Studio key set)", exc_info=e)
                return False
            return True # ADC seems configured and project ID is present
        else:  # OpenAI or other non-Gemini
            if not conf.api_key and not self.db.endpoint_override:
                if ctx:
                    txt = _("There are no API keys set for LLM interaction!\n")
                    if ctx.author.id == ctx.guild.owner_id: 
                        txt += _("- Set your OpenAI API key with `{cmd}assist openaikey`\n").format(cmd=ctx.clean_prefix)
                    await ctx.send(txt)
                return False
            return True


    async def resync_embeddings(self, conf: GuildSettings) -> int:
        """Update embeds to match current dimensions or model type."""
        if not conf.embeddings:
            return 0
        
        sample_key = list(conf.embeddings.keys())[0] if conf.embeddings else None
        if not sample_key:
            return 0 # No embeddings to sample or resync

        sample_text_for_probing = conf.embeddings[sample_key].text
        try:
            sample_embed_vector = await self.request_embedding(sample_text_for_probing, conf)
        except Exception as e:
            log.error(f"Failed to get sample embedding during resync: {e}. Aborting resync.")
            return 0

        target_dimension = len(sample_embed_vector)
        target_model_name = conf.embed_model 

        synced_count = 0
        tasks = []

        for name, em_data in conf.embeddings.items():
            if em_data.model != target_model_name or len(em_data.embedding) != target_dimension:
                log.info(f"Resyncing embedding for '{name}': model mismatch ('{em_data.model}' vs '{target_model_name}') or dim mismatch ({len(em_data.embedding)} vs {target_dimension}).")
                
                async def update_task(n, text_to_embed, current_conf):
                    try:
                        new_embedding_vector = await self.request_embedding(text_to_embed, current_conf)
                        current_conf.embeddings[n].embedding = new_embedding_vector
                        current_conf.embeddings[n].model = current_conf.embed_model 
                        current_conf.embeddings[n].update() 
                        log.debug(f"Successfully updated embedding for '{n}' to model '{current_conf.embed_model}'.")
                        return 1
                    except Exception as e_update:
                        log.error(f"Failed to update embedding for '{n}' during resync: {e_update}")
                        return 0

                tasks.append(update_task(name, em_data.text, conf))

        if tasks:
            results = await asyncio.gather(*tasks)
            synced_count = sum(results)
            if synced_count > 0:
                log.info(f"Resynced {synced_count} embeddings successfully.")
        
        return synced_count


    def get_max_tokens(self, conf: GuildSettings, user: Optional[discord.Member]) -> int:
        user_max_convo_tokens = conf.get_user_max_tokens(user) 
        model_name = conf.get_user_model(user)
        
        default_for_gemini = 1000000 if "1.5" in model_name or "2." in model_name else 32000
        max_model_total_tokens = MODELS.get(model_name, default_for_gemini if model_name.startswith("gemini-") else 4096)

        if not user_max_convo_tokens or user_max_convo_tokens > max_model_total_tokens:
            return max_model_total_tokens 
        return user_max_convo_tokens 

    async def cut_text_by_tokens(self, text: str, conf: GuildSettings, user: Optional[discord.Member] = None) -> str:
        if not text:
            log.debug("No text to cut by tokens!")
            return text
        
        model_name = conf.get_user_model(user)
        max_tokens_for_context = self.get_max_tokens(conf, user)
        tokens = await self.get_tokens(text, model_name) 
        cut_tokens = tokens[:max_tokens_for_context]
        return await self.get_text(cut_tokens, model_name) 

    async def get_text(self, tokens: list, model_name: str = "gpt-4o-mini") -> str:
        if model_name.startswith("gemini-"):
            try:
                return "".join([chr(t) for t in tokens])
            except TypeError: 
                log.error(f"Cannot decode Gemini tokens for model {model_name} as char codes. Tokens: {tokens[:10]}")
                return "" 

        def _get_encoding_openai():
            try:
                enc = tiktoken.encoding_for_model(model_name)
            except KeyError:
                log.warning(f"Tiktoken encoding not found for model {model_name}. Using o200k_base.")
                enc = tiktoken.get_encoding("o200k_base")
            return enc

        encoding = await asyncio.to_thread(_get_encoding_openai)
        return await asyncio.to_thread(encoding.decode, tokens, errors="ignore") 

    # -------------------------------------------------------
    # -------------------------------------------------------
    # -------------------- FORMATTING -----------------------
    # -------------------------------------------------------
    # -------------------------------------------------------
    async def degrade_conversation(
        self,
        messages: List[dict],
        function_list: List[dict],
        conf: GuildSettings,
        user: Optional[discord.Member],
    ) -> bool:
        """
        Iteratively degrade a conversation payload in-place to fit within the max token limit, prioritizing more recent messages and critical context.

        Order of importance:
        - System messages
        - Function calls available to model
        - Most recent user message
        - Most recent assistant message
        - Most recent function/tool message

        System messages are always ignored.

        Args:
            messages (List[dict]): message entries sent to the api
            function_list (List[dict]): list of json function schemas for the model
            conf: (GuildSettings): current settings

        Returns:
            bool: whether the conversation was degraded
        """
        # Fetch the current model the user is using
        model = conf.get_user_model(user)
        # Fetch the max token limit for the current user
        max_tokens = self.get_max_tokens(conf, user)
        # Token count of current conversation
        convo_tokens = await self.count_payload_tokens(messages, model)
        # Token count of function calls available to model
        function_tokens = await self.count_function_tokens(function_list, model)

        total_tokens = convo_tokens + function_tokens

        # Check if the total token count is already under the max token limit
        if total_tokens <= max_tokens:
            return False

        log.debug(f"Degrading messages for {user} (total: {total_tokens}/max: {max_tokens})")

        def count(role: str):
            return sum(1 for msg in messages if msg["role"] == role)

        async def pop(role: str) -> int:
            for idx, msg in enumerate(messages):
                if msg["role"] != role:
                    continue
                removed = messages.pop(idx)
                reduction = 4
                if "name" in removed:
                    reduction += 1
                if content := removed.get("content"):
                    if isinstance(content, list):
                        for i in content:
                            if i["type"] == "text":
                                reduction += await self.count_tokens(i["text"], model)
                            else:
                                reduction += 2
                    else:
                        reduction += await self.count_tokens(str(content), model)
                elif tool_calls := removed.get("tool_calls"):
                    reduction += await self.count_tokens(str(tool_calls), model)
                elif function_call := removed.get("function_call"):
                    reduction += await self.count_tokens(str(function_call), model)
                return reduction
            return 0

        # We will NOT remove the most recent user message or assistant message
        # We will also not touch system messages
        # We will also not touch function calls available to model (yet)
        iters = 0
        while True:
            iters += 1
            break_conditions = [
                count("user") <= 1,
                count("assistant") <= 1,
                iters > 100,
            ]
            if any(break_conditions):
                break
            # First we will iterate through the messages and remove in the following sweep order:
            # 1. Remove oldest tool call or response
            reduced = await pop("tool")
            if reduced:
                total_tokens -= reduced
                if total_tokens <= max_tokens:
                    break
            reduced = await pop("function")
            if reduced:
                total_tokens -= reduced
                if total_tokens <= max_tokens:
                    break
            # 2. Remove oldest assistant message
            reduced = await pop("assistant")
            if reduced:
                total_tokens -= reduced
                if total_tokens <= max_tokens:
                    break
            # 3. Remove oldest user message
            reduced = await pop("user")
            if reduced:
                total_tokens -= reduced
                if total_tokens <= max_tokens:
                    break
            # Then we will repeat the process until we are under the max token limit

        log.debug(f"Convo degradation finished for {user} (total: {total_tokens}/max: {max_tokens})")
        return True

    async def token_pagify(self, text: str, conf: GuildSettings) -> List[str]:
        """Pagify a long string by tokens rather than characters"""
        if not text:
            log.debug("No text to pagify!")
            return []
        token_chunks = []
        tokens = await self.get_tokens(text)
        current_chunk = []

        max_tokens = min(conf.max_tokens - 100, MODELS[conf.model])
        for token in tokens:
            current_chunk.append(token)
            if len(current_chunk) == max_tokens:
                token_chunks.append(current_chunk)
                current_chunk = []

        if current_chunk:
            token_chunks.append(current_chunk)

        text_chunks = []
        for chunk in token_chunks:
            text = await self.get_text(chunk)
            text_chunks.append(text)

        return text_chunks

    # -------------------------------------------------------
    # -------------------------------------------------------
    # ----------------------- EMBEDS ------------------------
    # -------------------------------------------------------
    # -------------------------------------------------------
    async def get_function_menu_embeds(self, user: discord.Member) -> List[discord.Embed]:
        func_dump = {k: v.model_dump(exclude_defaults=False) for k, v in self.db.functions.items()}
        registry = {"Assistant-Custom": func_dump}
        for cog_name, function_schemas in self.registry.items():
            cog = self.bot.get_cog(cog_name)
            if not cog:
                continue
            for function_name, data in function_schemas.items():
                function_schema = data["schema"]
                function_obj = getattr(cog, function_name, None)
                if function_obj is None:
                    continue
                if cog_name not in registry:
                    registry[cog_name] = {}
                registry[cog_name][function_name] = {
                    "code": inspect.getsource(function_obj),
                    "jsonschema": function_schema,
                    "permission_level": data["permission_level"],
                }

        conf = self.db.get_conf(user.guild)
        model = conf.get_user_model(user)

        pages = sum(len(v) for v in registry.values())
        page = 1
        embeds = []
        for cog_name, functions in registry.items():
            for function_name, data in functions.items():
                embed = discord.Embed(
                    title=_("Custom Functions"),
                    description=function_name,
                    color=discord.Color.blue(),
                )
                if cog_name != "Assistant-Custom":
                    embed.add_field(
                        name=_("3rd Party"),
                        value=_("This function is managed by the `{}` cog").format(cog_name),
                        inline=False,
                    )
                elif cog_name == "Assistant":
                    embed.add_field(
                        name=_("Internal Function"),
                        value=_("This is an internal command that can only be used when interacting with a tutor"),
                        inline=False,
                    )
                schema = json.dumps(data["jsonschema"], indent=2)
                tokens = await self.count_tokens(schema, model)

                schema_text = _("This function consumes `{}` input tokens each call\n").format(humanize_number(tokens))

                if user.id in self.bot.owner_ids:
                    if len(schema) > 900:
                        schema_text += box(schema[:900] + "...", "py")
                    else:
                        schema_text += box(schema, "py")

                    if len(data["code"]) > 900:
                        code_text = box(data["code"][:900] + "...", "py")
                    else:
                        code_text = box(data["code"], "py")

                else:
                    schema_text += box(data["jsonschema"]["description"], "json")
                    code_text = box(_("Hidden..."))

                embed.add_field(name=_("Permission Level"), value=data["permission_level"].capitalize(), inline=False)
                embed.add_field(name=_("Schema"), value=schema_text, inline=False)
                embed.add_field(name=_("Code"), value=code_text, inline=False)

                embed.set_footer(text=_("Page {}/{}").format(page, pages))
                embeds.append(embed)
                page += 1

        if not embeds:
            embeds.append(
                discord.Embed(
                    description=_("No custom code has been added yet!"),
                    color=discord.Color.purple(),
                )
            )
        return embeds

    async def get_embbedding_menu_embeds(self, conf: GuildSettings, place: int) -> List[discord.Embed]:
        embeddings = sorted(conf.embeddings.items(), key=lambda x: x[0])
        embeds = []
        pages = math.ceil(len(embeddings) / 5)
        model = conf.get_user_model()
        start = 0
        stop = 5
        for page in range(pages):
            stop = min(stop, len(embeddings))
            embed = discord.Embed(title=_("Embeddings"), color=discord.Color.blue())
            embed.set_footer(text=_("Page {}/{}").format(page + 1, pages))
            num = 0
            for i in range(start, stop):
                name, embedding = embeddings[i]
                tokens = await self.count_tokens(embedding.text, model)
                text = (
                    box(f"{embedding.text[:30].strip()}...")
                    if len(embedding.text) > 33
                    else box(embedding.text.strip())
                )
                val = _(
                    "`Created:    `{}\n"
                    "`Modified:   `{}\n"
                    "`Tokens:     `{}\n"
                    "`Dimensions: `{}\n"
                    "`AI Created: `{}\n"
                    "`Model:      `{}\n"
                ).format(
                    embedding.created_at(),
                    embedding.modified_at(relative=True),
                    tokens,
                    len(embedding.embedding),
                    embedding.ai_created,
                    conf.embed_model,
                )
                val += text
                fieldname = f"➣ {name}" if place == num else name
                embed.add_field(
                    name=fieldname[:250],
                    value=val,
                    inline=False,
                )
                num += 1
            embeds.append(embed)
            start += 5
            stop += 5
        if not embeds:
            embeds.append(discord.Embed(description=_("No embeddings have been added!"), color=discord.Color.purple()))
        return embeds
